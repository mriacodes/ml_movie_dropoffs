{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd89a2b",
   "metadata": {},
   "source": [
    "# Model Deployment & Export\n",
    "\n",
    "This notebook handles the export of the trained model for production deployment.\n",
    "It creates .pkl files and deployment utilities for both Django and FastAPI frameworks.\n",
    "\n",
    "**Prerequisites**: Run `model-building-evaluation.ipynb` first to train and select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d5285",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set working directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "os.chdir('C:/Users/oldbe/Machine Learning/survey-seeding/backend')\n",
    "print(\"Changed to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and recreate the model selection results\n",
    "# This assumes you've run the model-building-evaluation notebook\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('model_framing_assembling/ml_dataset_final.csv', index_col=0)\n",
    "target_column = 'target_dropoff'  \n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f991730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the train-test split and SMOTE application\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Split data (same as in evaluation notebook)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE+Tomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote_tomek.fit_resample(X_train, y_train)\n",
    "X_train_balanced = pd.DataFrame(X_train_balanced, columns=X_train.columns)\n",
    "\n",
    "print(f\"Training data prepared:\")\n",
    "print(f\"  Original: {len(X_train)} samples\")\n",
    "print(f\"  Balanced: {len(X_train_balanced)} samples\")\n",
    "print(f\"  Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5798c97",
   "metadata": {},
   "source": [
    "## 2. Train Best Model for Export\n",
    "\n",
    "Based on the evaluation results, we'll train the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1123a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models and train them to determine the best one\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Define models (same configuration as evaluation)\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50),\n",
    "        early_stopping=True, validation_fraction=0.1\n",
    "    ),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "print(\"üöÄ Training models for deployment export...\")\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train on balanced data\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {results[name]['Accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {results[name]['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9653dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model using weighted scoring\n",
    "weights = {\n",
    "    'F1-Score': 0.30,\n",
    "    'Accuracy': 0.25,\n",
    "    'ROC-AUC': 0.20,\n",
    "    'Precision': 0.15,\n",
    "    'Recall': 0.10\n",
    "}\n",
    "\n",
    "# Calculate weighted scores\n",
    "weighted_scores = {}\n",
    "for model_name in results.keys():\n",
    "    score = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for metric, weight in weights.items():\n",
    "        if metric == 'ROC-AUC' and results[model_name][metric] is None:\n",
    "            continue\n",
    "        \n",
    "        score += results[model_name][metric] * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    weighted_scores[model_name] = score / total_weight if total_weight > 0 else 0\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(weighted_scores.keys(), key=lambda k: weighted_scores[k])\n",
    "best_model = trained_models[best_model_name]\n",
    "best_score = weighted_scores[best_model_name]\n",
    "\n",
    "print(f\"üèÜ BEST MODEL SELECTED: {best_model_name}\")\n",
    "print(f\"Weighted Score: {best_score:.4f}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "for metric, value in results[best_model_name].items():\n",
    "    if value is not None:\n",
    "        print(f\"  {metric:12s}: {value:.4f} ({value:.1%})\")\n",
    "    else:\n",
    "        print(f\"  {metric:12s}: N/A\")\n",
    "\n",
    "# Check if target is met\n",
    "target_accuracy = 0.70\n",
    "accuracy_achieved = results[best_model_name]['Accuracy']\n",
    "target_met = accuracy_achieved >= target_accuracy\n",
    "\n",
    "print(f\"\\nüéØ TARGET ACHIEVEMENT:\")\n",
    "if target_met:\n",
    "    print(f\"‚úÖ EXCEEDS 70% accuracy target: {accuracy_achieved:.1%}\")\n",
    "else:\n",
    "    print(f\"‚ùå Below 70% accuracy target: {accuracy_achieved:.1%}\")\n",
    "    print(f\"‚ö†Ô∏è  Proceeding with best available model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9863a8",
   "metadata": {},
   "source": [
    "## 3. Export Model Files\n",
    "\n",
    "### 3.1 Create Export Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aace513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for different deployment frameworks\n",
    "export_dirs = {\n",
    "    'fastapi': 'fastapi_models',\n",
    "    'general': 'exported_models'  # For general use\n",
    "}\n",
    "\n",
    "for framework, dir_name in export_dirs.items():\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"‚úÖ Created directory: {dir_name}\")\n",
    "\n",
    "print(f\"\\nüìÅ Export directories ready for deployment files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b4bed",
   "metadata": {},
   "source": [
    "### 3.2 Export Core Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model and preprocessing components\n",
    "export_timestamp = datetime.now().isoformat()\n",
    "\n",
    "print(\"üì¶ EXPORTING MODEL FOR PRODUCTION DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export to both directories\n",
    "for framework, models_dir in export_dirs.items():\n",
    "    print(f\"\\nExporting to {framework.upper()} directory: {models_dir}/\")\n",
    "    \n",
    "    # 1. Export the trained model\n",
    "    model_filename = f\"{models_dir}/movie_dropoff_model.pkl\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"‚úÖ Model: {model_filename}\")\n",
    "    \n",
    "    # 2. Export SMOTE+Tomek transformer\n",
    "    smote_filename = f\"{models_dir}/smote_transformer.pkl\"\n",
    "    joblib.dump(smote_tomek, smote_filename)\n",
    "    print(f\"‚úÖ SMOTE transformer: {smote_filename}\")\n",
    "    \n",
    "    # 3. Export model metadata\n",
    "    model_info = {\n",
    "        'model_type': best_model_name,\n",
    "        'model_class': str(type(best_model).__name__),\n",
    "        'training_date': export_timestamp,\n",
    "        'framework': framework,\n",
    "        'target_column': target_column,\n",
    "        'feature_names': X.columns.tolist(),\n",
    "        'feature_count': len(X.columns),\n",
    "        'performance_metrics': results[best_model_name],\n",
    "        'weighted_score': best_score,\n",
    "        'target_achieved': target_met,\n",
    "        'training_samples': {\n",
    "            'original': len(X_train),\n",
    "            'balanced': len(X_train_balanced),\n",
    "            'test': len(X_test)\n",
    "        },\n",
    "        'preprocessing': {\n",
    "            'balancing_method': 'SMOTE+Tomek',\n",
    "            'train_test_split': 0.25,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'version': '1.0',\n",
    "        'export_timestamp': export_timestamp\n",
    "    }\n",
    "    \n",
    "    info_filename = f\"{models_dir}/model_info.json\"\n",
    "    with open(info_filename, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Model info: {info_filename}\")\n",
    "    \n",
    "    # 4. Export class mappings\n",
    "    class_mappings = {\n",
    "        'label_mapping': {\n",
    "            '0': 'Will Complete Movie',\n",
    "            '1': 'Will Drop Off',\n",
    "            0: 'Will Complete Movie',\n",
    "            1: 'Will Drop Off'\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'original': {\n",
    "                'class_0': int(sum(y == 0)),\n",
    "                'class_1': int(sum(y == 1))\n",
    "            },\n",
    "            'training_balanced': {\n",
    "                'class_0': int(sum(y_train_balanced == 0)),\n",
    "                'class_1': int(sum(y_train_balanced == 1))\n",
    "            }\n",
    "        },\n",
    "        'target_names': ['Complete', 'Dropout']\n",
    "    }\n",
    "    \n",
    "    mappings_filename = f\"{models_dir}/class_mappings.json\"\n",
    "    with open(mappings_filename, 'w') as f:\n",
    "        json.dump(class_mappings, f, indent=2)\n",
    "    print(f\"‚úÖ Class mappings: {mappings_filename}\")\n",
    "\n",
    "print(f\"\\nüéâ Model export completed successfully!\")\n",
    "print(f\"Exported model: {best_model_name} with {accuracy_achieved:.1%} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42032880",
   "metadata": {},
   "source": [
    "### 3.3 Export Survey Questions for Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate survey questions based on the features used in the model\n",
    "# This creates a comprehensive survey form for web applications\n",
    "\n",
    "survey_questions = [\n",
    "    {\n",
    "        'id': 'age_group',\n",
    "        'question': 'What is your age group?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': '18-25', 'label': '18-25 years'},\n",
    "            {'value': '26-35', 'label': '26-35 years'},\n",
    "            {'value': '36-45', 'label': '36-45 years'},\n",
    "            {'value': '46-55', 'label': '46-55 years'},\n",
    "            {'value': '56+', 'label': '56+ years'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'viewing_frequency',\n",
    "        'question': 'How often do you watch movies?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Daily', 'label': 'Daily'},\n",
    "            {'value': 'Weekly', 'label': 'Several times a week'},\n",
    "            {'value': 'Monthly', 'label': 'A few times a month'},\n",
    "            {'value': 'Rarely', 'label': 'Rarely'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'preferred_genre',\n",
    "        'question': 'What is your preferred movie genre?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Action', 'label': 'Action'},\n",
    "            {'value': 'Comedy', 'label': 'Comedy'},\n",
    "            {'value': 'Drama', 'label': 'Drama'},\n",
    "            {'value': 'Horror', 'label': 'Horror'},\n",
    "            {'value': 'Romance', 'label': 'Romance'},\n",
    "            {'value': 'Sci-Fi', 'label': 'Science Fiction'},\n",
    "            {'value': 'Thriller', 'label': 'Thriller'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'attention_span',\n",
    "        'question': 'How would you describe your attention span for movies?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Short', 'label': 'Short (prefer movies under 90 minutes)'},\n",
    "            {'value': 'Medium', 'label': 'Medium (comfortable with 90-120 minutes)'},\n",
    "            {'value': 'Long', 'label': 'Long (enjoy movies over 2 hours)'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'device_preference',\n",
    "        'question': 'What device do you primarily use to watch movies?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'TV', 'label': 'Television'},\n",
    "            {'value': 'Laptop', 'label': 'Laptop/Computer'},\n",
    "            {'value': 'Tablet', 'label': 'Tablet'},\n",
    "            {'value': 'Phone', 'label': 'Smartphone'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'viewing_companion',\n",
    "        'question': 'Who do you usually watch movies with?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Alone', 'label': 'Alone'},\n",
    "            {'value': 'Partner', 'label': 'Partner/Spouse'},\n",
    "            {'value': 'Family', 'label': 'Family'},\n",
    "            {'value': 'Friends', 'label': 'Friends'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'interruption_tolerance',\n",
    "        'question': 'How well do you handle interruptions while watching movies?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Low', 'label': 'Low - I prefer no interruptions'},\n",
    "            {'value': 'Medium', 'label': 'Medium - Some interruptions are okay'},\n",
    "            {'value': 'High', 'label': 'High - Interruptions don\\'t bother me'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'mood_influence',\n",
    "        'question': 'How much does your mood influence your movie completion?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'High', 'label': 'High - My mood greatly affects viewing'},\n",
    "            {'value': 'Medium', 'label': 'Medium - Some mood influence'},\n",
    "            {'value': 'Low', 'label': 'Low - Mood rarely affects viewing'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'content_discovery',\n",
    "        'question': 'How do you usually discover movies to watch?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Recommendations', 'label': 'Platform recommendations'},\n",
    "            {'value': 'Browse', 'label': 'Browsing categories'},\n",
    "            {'value': 'Search', 'label': 'Searching for specific titles'},\n",
    "            {'value': 'Social', 'label': 'Social media/friends'}\n",
    "        ],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'id': 'time_of_day',\n",
    "        'question': 'When do you prefer to watch movies?',\n",
    "        'type': 'select',\n",
    "        'options': [\n",
    "            {'value': 'Morning', 'label': 'Morning (6 AM - 12 PM)'},\n",
    "            {'value': 'Afternoon', 'label': 'Afternoon (12 PM - 6 PM)'},\n",
    "            {'value': 'Evening', 'label': 'Evening (6 PM - 10 PM)'},\n",
    "            {'value': 'Night', 'label': 'Night (10 PM - 2 AM)'}\n",
    "        ],\n",
    "        'required': True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Export survey questions to both directories\n",
    "for framework, models_dir in export_dirs.items():\n",
    "    questions_filename = f\"{models_dir}/survey_questions.json\"\n",
    "    with open(questions_filename, 'w') as f:\n",
    "        json.dump(survey_questions, f, indent=2)\n",
    "    print(f\"‚úÖ Survey questions exported: {questions_filename}\")\n",
    "\n",
    "print(f\"\\nüìù Survey form ready with {len(survey_questions)} questions\")\n",
    "print(\"Questions cover key behavioral and preference factors for dropout prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafc7d1",
   "metadata": {},
   "source": [
    "## 4. Export Training Data for Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e723e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export datasets for potential model retraining\n",
    "data_export_dir = 'exported_datasets'\n",
    "os.makedirs(data_export_dir, exist_ok=True)\n",
    "\n",
    "print(\"üìä EXPORTING TRAINING DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Export original training set\n",
    "X_train_export = X_train.copy()\n",
    "X_train_export[target_column] = y_train\n",
    "train_file = f\"{data_export_dir}/training_set_original.csv\"\n",
    "X_train_export.to_csv(train_file, index=False)\n",
    "print(f\"‚úÖ Original training set: {train_file} ({len(X_train_export)} samples)\")\n",
    "\n",
    "# 2. Export balanced training set\n",
    "X_train_balanced_export = X_train_balanced.copy()\n",
    "X_train_balanced_export[target_column] = y_train_balanced\n",
    "balanced_file = f\"{data_export_dir}/training_set_smote_balanced.csv\"\n",
    "X_train_balanced_export.to_csv(balanced_file, index=False)\n",
    "print(f\"‚úÖ Balanced training set: {balanced_file} ({len(X_train_balanced_export)} samples)\")\n",
    "\n",
    "# 3. Export test set\n",
    "X_test_export = X_test.copy()\n",
    "X_test_export[target_column] = y_test\n",
    "test_file = f\"{data_export_dir}/test_set.csv\"\n",
    "X_test_export.to_csv(test_file, index=False)\n",
    "print(f\"‚úÖ Test set: {test_file} ({len(X_test_export)} samples)\")\n",
    "\n",
    "# 4. Export full dataset\n",
    "full_file = f\"{data_export_dir}/full_dataset_original.csv\"\n",
    "df.to_csv(full_file, index=False)\n",
    "print(f\"‚úÖ Full dataset: {full_file} ({len(df)} samples)\")\n",
    "\n",
    "# Create dataset summary\n",
    "dataset_summary = {\n",
    "    'export_timestamp': export_timestamp,\n",
    "    'datasets': {\n",
    "        'full_original': {\n",
    "            'file': full_file,\n",
    "            'samples': len(df),\n",
    "            'features': len(X.columns),\n",
    "            'class_distribution': y.value_counts().to_dict()\n",
    "        },\n",
    "        'training_original': {\n",
    "            'file': train_file,\n",
    "            'samples': len(X_train),\n",
    "            'features': len(X_train.columns),\n",
    "            'class_distribution': y_train.value_counts().to_dict()\n",
    "        },\n",
    "        'training_balanced': {\n",
    "            'file': balanced_file,\n",
    "            'samples': len(X_train_balanced),\n",
    "            'features': len(X_train_balanced.columns),\n",
    "            'class_distribution': pd.Series(y_train_balanced).value_counts().to_dict(),\n",
    "            'balancing_method': 'SMOTE+Tomek'\n",
    "        },\n",
    "        'test_set': {\n",
    "            'file': test_file,\n",
    "            'samples': len(X_test),\n",
    "            'features': len(X_test.columns),\n",
    "            'class_distribution': y_test.value_counts().to_dict()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = f\"{data_export_dir}/dataset_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(dataset_summary, f, indent=2, default=str)\n",
    "print(f\"‚úÖ Dataset summary: {summary_file}\")\n",
    "\n",
    "print(f\"\\nüìÅ All datasets exported to: {data_export_dir}/\")\n",
    "print(\"These files can be used for model retraining and validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1e96c",
   "metadata": {},
   "source": [
    "## 5. Deployment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all exported files can be loaded correctly\n",
    "print(\"üîç DEPLOYMENT VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "verification_passed = True\n",
    "\n",
    "for framework, models_dir in export_dirs.items():\n",
    "    print(f\"\\nVerifying {framework.upper()} exports:\")\n",
    "    \n",
    "    try:\n",
    "        # Test model loading\n",
    "        loaded_model = joblib.load(f\"{models_dir}/movie_dropoff_model.pkl\")\n",
    "        print(f\"  ‚úÖ Model loads successfully: {type(loaded_model).__name__}\")\n",
    "        \n",
    "        # Test SMOTE transformer loading\n",
    "        loaded_smote = joblib.load(f\"{models_dir}/smote_transformer.pkl\")\n",
    "        print(f\"  ‚úÖ SMOTE transformer loads successfully\")\n",
    "        \n",
    "        # Test model info loading\n",
    "        with open(f\"{models_dir}/model_info.json\", 'r') as f:\n",
    "            model_info = json.load(f)\n",
    "        print(f\"  ‚úÖ Model info loads successfully\")\n",
    "        \n",
    "        # Test class mappings loading\n",
    "        with open(f\"{models_dir}/class_mappings.json\", 'r') as f:\n",
    "            class_mappings = json.load(f)\n",
    "        print(f\"  ‚úÖ Class mappings load successfully\")\n",
    "        \n",
    "        # Test survey questions loading\n",
    "        with open(f\"{models_dir}/survey_questions.json\", 'r') as f:\n",
    "            questions = json.load(f)\n",
    "        print(f\"  ‚úÖ Survey questions load successfully ({len(questions)} questions)\")\n",
    "        \n",
    "        # Test prediction with sample data\n",
    "        sample_input = X_test.iloc[:1]\n",
    "        prediction = loaded_model.predict(sample_input)\n",
    "        probability = loaded_model.predict_proba(sample_input) if hasattr(loaded_model, 'predict_proba') else None\n",
    "        print(f\"  ‚úÖ Model prediction works: {prediction[0]} (prob: {probability[0] if probability is not None else 'N/A'})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Verification failed: {str(e)}\")\n",
    "        verification_passed = False\n",
    "\n",
    "# Verify dataset exports\n",
    "print(f\"\\nVerifying dataset exports:\")\n",
    "dataset_files = [\n",
    "    f\"{data_export_dir}/training_set_original.csv\",\n",
    "    f\"{data_export_dir}/training_set_smote_balanced.csv\",\n",
    "    f\"{data_export_dir}/test_set.csv\",\n",
    "    f\"{data_export_dir}/full_dataset_original.csv\"\n",
    "]\n",
    "\n",
    "for file_path in dataset_files:\n",
    "    try:\n",
    "        test_df = pd.read_csv(file_path)\n",
    "        print(f\"  ‚úÖ {os.path.basename(file_path)}: {test_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {os.path.basename(file_path)}: {str(e)}\")\n",
    "        verification_passed = False\n",
    "\n",
    "print(f\"\\n{'üéâ ALL VERIFICATIONS PASSED!' if verification_passed else '‚ùå SOME VERIFICATIONS FAILED!'}\")\n",
    "if verification_passed:\n",
    "    print(\"Model is ready for production deployment!\")\n",
    "else:\n",
    "    print(\"Please check the failed components before deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90036eb1",
   "metadata": {},
   "source": [
    "## 6. Deployment Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final deployment summary\n",
    "print(\"üöÄ DEPLOYMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã MODEL DETAILS:\")\n",
    "print(f\"  Selected Model: {best_model_name}\")\n",
    "print(f\"  Accuracy: {accuracy_achieved:.1%}\")\n",
    "print(f\"  F1-Score: {results[best_model_name]['F1-Score']:.3f}\")\n",
    "print(f\"  Target Achievement: {'‚úÖ EXCEEDED' if target_met else '‚ùå Below target'}\")\n",
    "print(f\"  Export Date: {export_timestamp}\")\n",
    "\n",
    "print(f\"\\nüìÅ EXPORTED FILES:\")\n",
    "\n",
    "# List FastAPI files\n",
    "print(f\"\\n  FastAPI Deployment ({export_dirs['fastapi']}):\")\n",
    "fastapi_files = [\n",
    "    \"movie_dropoff_model.pkl - Trained model\",\n",
    "    \"smote_transformer.pkl - Data preprocessing\", \n",
    "    \"model_info.json - Model metadata\",\n",
    "    \"class_mappings.json - Label mappings\",\n",
    "    \"survey_questions.json - Frontend form questions\",\n",
    "    \"fastapi_prediction_utils.py - API utilities (pre-created)\",\n",
    "    \"fastapi_retraining_utils.py - Retraining system (pre-created)\",\n",
    "    \"requirements.txt - Dependencies (pre-created)\"\n",
    "]\n",
    "for file_desc in fastapi_files:\n",
    "    print(f\"    ‚Ä¢ {file_desc}\")\n",
    "\n",
    "# List general export files\n",
    "print(f\"\\n  General Export ({export_dirs['general']}):\")\n",
    "general_files = [\n",
    "    \"movie_dropoff_model.pkl - Trained model\",\n",
    "    \"smote_transformer.pkl - Data preprocessing\",\n",
    "    \"model_info.json - Model metadata\", \n",
    "    \"class_mappings.json - Label mappings\",\n",
    "    \"survey_questions.json - Frontend form questions\"\n",
    "]\n",
    "for file_desc in general_files:\n",
    "    print(f\"    ‚Ä¢ {file_desc}\")\n",
    "\n",
    "# List dataset files\n",
    "print(f\"\\n  Training Data ({data_export_dir}):\")\n",
    "data_files = [\n",
    "    \"training_set_original.csv - Original training data\",\n",
    "    \"training_set_smote_balanced.csv - Balanced training data\",\n",
    "    \"test_set.csv - Test/validation data\",\n",
    "    \"full_dataset_original.csv - Complete dataset\",\n",
    "    \"dataset_summary.json - Data statistics\"\n",
    "]\n",
    "for file_desc in data_files:\n",
    "    print(f\"    ‚Ä¢ {file_desc}\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDED NEXT STEPS:\")\n",
    "next_steps = [\n",
    "    \"Review the FastAPI implementation guide (FASTAPI_IMPLEMENTATION_GUIDE.md)\",\n",
    "    \"Set up production environment with required dependencies\",\n",
    "    \"Deploy FastAPI application using uvicorn or container deployment\",\n",
    "    \"Implement frontend integration using the survey questions JSON\",\n",
    "    \"Set up monitoring and logging for prediction requests\",\n",
    "    \"Configure automated retraining pipeline with user feedback\",\n",
    "    \"Perform A/B testing to validate prediction effectiveness\",\n",
    "    \"Monitor model performance and retrain as needed\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(f\"\\n‚ú® Model deployment package ready!\")\n",
    "print(f\"üîó Use the FastAPI directory for immediate web deployment\")\n",
    "print(f\"üìö Refer to the implementation guides for detailed setup instructions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
